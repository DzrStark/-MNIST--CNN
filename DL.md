[TOC]



##  一、背景知识

### 1.1 机器学习基础

​	**机器学习的核心是通过数据来训练模型**，使其能够找到数据中隐藏的规律，从而对未知数据做出有效的预测。在机器学习中，模型通过学习输入的数据来生成输出。在**训练**过程中，模型根据输入数据的特征和标签之间的关系来调整参数，以便更好地拟合数据。当模型在训练数据上表现良好时，可以利用它对**测试**数据进行预测，进而评估模型性能。

​	机器学习主要分为三大类：**监督学习**、**无监督学习**和**强化学习**。在**监督学习**中，训练数据是**有标签**的，即每个输入数据都有一个已知的输出标签，模型的目标是学习输入与输出之间的映射关系，进而对新数据进行预测。**无监督学习**则**不依赖于标签数据**，模型通过数据间的相似性或某种潜在结构来进行学习，常见任务如聚类或降维。而**强化学习**则是一**种基于奖励和惩罚的学习方法**，通常应用于需要与环境进行交互的任务，如机器人控制或游戏。

![image-20250101222148838](C:\Users\DcStark\AppData\Roaming\Typora\typora-user-images\image-20250101222148838.png)

### 1.2 深度学习基础

​	个人认为，**深度学习**属于机器学习中的**监督学习**。因为深度学习模型通常是通过带标签的训练数据进行训练的，这恰恰符合监督学习的定义。在本次的**图像分类任务**中，深度学习模型（如我们本次使用的**卷积神经网络CNN**）通过**带有标签的图像数据集MNIST**进行训练，每一张28x28像素的图像都有一个对应的数字标签（0-9）。通过这些带标签的数据，深度学习模型能够在训练过程中反复调整其网络结构和参数，使得模型逐渐学习到如何从图像的像素数据中提取出能准确区分各个数字的特征。

​	与传统机器学习方法不同，**深度学习优势在于它能自动从数据中提取特征**，而无需手动设计和选择特征。虽然深度学习能够自动从数据中提取特征，减少了人工设计特征的工作量，但它仍然需要**人工设定**一些关键的**超参数**。**超参数**是指在模型训练前需要手动设置的参数，**不同于模型通过数据自动学习的参数**。深度学习模型的超参数包括**学习率、批量大小、网络的层数、每层的神经元数量、激活函数的选择等**。这些超参数对模型的性能和训练效率有着重要影响，正确的超参数设置可以显著提升模型的效果，而不合适的超参数则可能导致模型收敛缓慢，甚至无法收敛。比如，学习率过大可能导致训练过程中的震荡，而过小则可能使训练进程非常缓慢。网络层数和每层神经元的数量也影响模型的表示能力，层数过少可能导致模型无法捕捉复杂的特征，而层数过多则可能导致过拟合或训练时间过长。因此，尽管深度学习能够自动从数据中学习特征，**超参数的选择依然是深度学习中一个不可忽视的重要环节**，这也将会是我们此次实验的重点。

​	我觉得下面这张图很好的解释了**机器学习和深度学习的区别**，有点糊，请老师见谅。其实重点就在于**深度学习简化了人工特征提取**这一步。

![image-20250101222007593](C:\Users\DcStark\AppData\Roaming\Typora\typora-user-images\image-20250101222007593.png)

### 1.3 卷积神经网络CNN与MNIST数据集

#### 1.3.1 卷积神经网络CNN

​	**卷积神经网络（Convolutional Neural Network，简称CNN）**是一类深度学习模型，专门用于处理具有网格结构的数据，尤其是图像。CNN的核心思想是模仿人类视觉神经系统的工作原理，通过多层次的卷积运算来提取图像中的特征信息，并最终进行分类或回归任务。

CNN主要由以下几种层组成：

1. **卷积层**：

- 这是CNN的核心层，主要通过卷积操作提取图像的局部特征。

- 卷积核通过滑动窗口的方式，逐步扫描输入图像并进行局部特征的学习。每个卷积核学习到的特征反映了图像中的特定模式。

2. **激活函数**：

- 在卷积操作后，通常使用激活函数将线性输出转化为非线性输出，以增加模型的表达能力和非线性特征。
- ReLU是一种常用的激活函数，其优点是计算简单且能有效缓解梯度消失问题。

3. **池化层**：

- 池化操作用于减少特征图的尺寸，从而降低计算量并避免过拟合。池化操作通常有最大池化（Max Pooling）和平均池化（Average Pooling）两种方式。
- 池化能够使得模型更加鲁棒，对输入图像的平移、旋转、缩放等变换具有一定的容忍度。

4. **全连接层**：

- 在网络的最后阶段，经过卷积和池化处理后的特征图会被展开并输入到全连接层，进行最终的分类或回归操作。
- 全连接层中的神经元与前一层的所有神经元相连接，是一个密集的连接结构，用于综合所有特征并进行决策。

5. **输出层**：

- 输出层根据任务的不同决定输出形式。对于分类任务，输出层通常使用softmax激活函数，输出一个表示不同类别概率的向量。

​	基本的CNN网络如下图所示，这张图画的比较好。

​	下文我介绍自己搭建的CNN网络时会配有我专门为此次实验搭建的可视化网络图片。

![image-20250101222931642](C:\Users\DcStark\AppData\Roaming\Typora\typora-user-images\image-20250101222931642.png)

#### 1.3.2 MNIST数据集

​	MNIST是一个广泛使用的手写数字识别数据集。作为深度学习和机器学习领域的经典数据集，MNIST常用于分类算法的训练和测试。它包含了手写数字图像，分别表示0到9这10个类别，并经过标准化和预处理，易于快速上手，是初学者理解和实践机器学习与深度学习的入门数据集。

​	数据集由两个主要部分组成：**训练集和测试集**。训练集包含**60,000张28x28像素的灰度图像**，测试集包含**10,000张**相同尺寸的图像。每张图像的像素值范围从0到255，0表示黑色，255表示白色，中间的值表示不同的灰度级别。每张图像都有一个**标签**，表示该图像所代表的手写数字的真实类别（从0到9）。这个数据集的标准化和简化使得它成为机器学习研究中的一个理想选择。

​	此次实验代码中的训练集展示：

![image-20250101223333741](C:\Users\DcStark\AppData\Roaming\Typora\typora-user-images\image-20250101223333741.png)

## 二、研究目的

1. **实现手写数字识别**：
   	利用**MNIST数据集**，编程实现手写数字的识别，完成从图像数据到数字标签的映射任务。该实验旨在深入理解机器学习与深度学习中分类模型的基本构建流程，包括数据预处理、模型设计、训练和测试等核心步骤。我将利用**Pytorch框架**和**CNN网络**来解决**手写数字识别**问题。

2. **探讨超参数对模型性能的影响**：
   	对比并分析**不同训练轮数、学习率、隐藏神经元数量和批处理大小等超参数**对模型性能的影响。通过调整这些超参数并观察模型的训练结果，探讨其对模型收敛速度、预测精度以及泛化能力的作用。加深对超参数调节的理解，进一步提升对深度学习模型复杂性的认识和掌控力。

## 三、研究内容

### 3.1 数据预处理

```python
# MNIST测试集
train_dataset = datasets.MNIST(root='.', train=True, transform=transforms.ToTensor(), download=download)
train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=batch_size)

# 存入迭代器
dataiter = iter(train_loader)
batch = next(dataiter)
imshow(make_grid(batch[0],nrow=10,padding=2,pad_value=1),'训练集数据')
```

​	在这段代码中，数据预处理主要是通过PyTorch提供的`datasets.MNIST`和`transforms.ToTensor()`来完成的。首先，`datasets.MNIST`用于加载MNIST数据集。`transform=transforms.ToTensor()`是数据预处理的关键，它将每个图像从**NumPy数组格式转换为Tensor格式**，并且将图像的像素值从原来的[0, 255]的范围映射到[0.0, 1.0]之间，这样可以避免训练过程中出现**梯度不稳定或模型训练效率低下**的情况。

​	接下来，使用`DataLoader`将数据集分成小批次以便进行训练。在此过程中，设置`shuffle=True`会在每次训练前对数据集进行洗牌，这有助于避免模型对数据的顺序产生依赖，从而提高**模型的泛化能力**。同时，`batch_size=batch_size`指定了每个批次的数据量，这里设置为100，这样每次训练时，模型将使用100个样本进行一次参数更新。

​	简单来说，在训练过程中，每次从`train_loader`中获取一个批次的数据。数据通常是一个包含图像和标签的元组，图像数据是一个四维Tensor，其形状为`(batch_size, 1, 28, 28)`。为了方便查看训练数据，代码中使用`make_grid`函数将一个批次的图像拼接成一个大的图像网格，并通过`imshow`函数显示出来。

​	对于测试集的处理方式也与训练集类似，同样使用`datasets.MNIST`加载测试数据，并通过`DataLoader`分批次加载。通过这些步骤，数据被转换为模型训练所需要的格式，并为后续的训练和测试做好了准备。



> 值得一提的是，我这里设置了**图像的显示函数imshow**。

```python
def imshow(img,title=None):
    npimg = img.numpy()
    plt.axis("off")
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.title(title)
    plt.show()
```

​	此部分函数就不多做介绍了。==值得注意的是，由于`plt.show`函数的存在，我们展示完训练集数据（如下图），需要手动关闭，后面的几张图皆是如此，程序才能继续进行。==

![image-20250101232255069](C:\Users\DcStark\AppData\Roaming\Typora\typora-user-images\image-20250101232255069.png)

### 3.2 模型的分析与构建

```python
class MNIST_Network(nn.Module):
    def __init__(self):
        super(MNIST_Network, self).__init__()

        self.conv1 = nn.Conv2d(1,32,kernel_size = 5,padding=2)  # 卷积层
        self.relu1 = nn.ReLU()                                  # 激活函数ReLU
        self.pool1 = nn.MaxPool2d(2,stride=2)                   # 最大池化层

        self.conv2 = nn.Conv2d(32,64,kernel_size = 5,padding=2) # 卷积层
        self.relu2 = nn.ReLU()                                  # 激活函数ReLU
        self.pool2 = nn.MaxPool2d(2,stride=2)                   # 最大池化层

        self.fc3 = nn.Linear(7*7*64,1024)                       # 全连接层
        self.relu3 = nn.ReLU()                                  # 激活函数ReLU

        self.fc4 = nn.Linear(1024,10)                           # 全连接层
        self.softmax4 = nn.Softmax(dim=1)                       # Softmax层

    # 前向传播
    def forward(self, input1):
        x = self.conv1(input1)
        x = self.relu1(x)
        x = self.pool1(x)

        x = self.conv2(x)
        x = self.relu2(x)
        x = self.pool2(x)

        x = x.view(x.size()[0], -1)
        x = self.fc3(x)
        x = self.relu3(x)

        x = self.fc4(x)
        x = self.softmax4(x)
        return x
```

​	在本次实验中，我构建了一个**卷积神经网络（CNN）模型**来实现对MNIST手写数字数据集进行分类。该网络模型被定义在`MNIST_Network`类中。CNN网络结构主要由**卷积层、池化层、全连接层和Softmax层**组成，旨在逐步提取图像特征并进行分类。

​	具体来说，网络的第一层是一个卷积层`conv1`，它使用5x5的卷积核对输入图像进行卷积操作，并设置了`padding=2`以保持输入图像的尺寸。该卷积层的输出通道数为32，即提取32个特征图。接着，`relu1`层是一个ReLU激活函数，能够引入非线性，从而提高网络的表达能力。随后，`pool1`层是一个最大池化层，采用2x2的池化核和步幅为2的配置，目的在于对特征图进行下采样，减少数据量，同时保留最重要的特征。

​	第二组卷积层包括`conv2`、`relu2`和`pool2`，它们分别进行卷积、激活和池化操作。`conv2`的输入是前一层池化后的32个通道，输出64个特征图，进一步增强了网络对图像细节的提取能力。类似于第一组卷积层，`relu2`和`pool2`分别执行激活和池化操作。

​	在卷积层和池化层提取到足够的特征后，网络将图像展平（`x.view(x.size()[0], -1)`），然后通过全连接层`fc3`进行特征映射。`fc3`层的输出维度为1024，这一层有助于捕捉更加复杂的特征关系。接下来，`relu3`激活函数增加了非线性。

​	最后，通过`fc4`层，网络输出10个神经元，对应MNIST数据集中的10个数字类别。最后，Softmax激活函数`softmax4`将网络的输出转化为概率分布，表示图像属于每个类别的概率。

​	总结而言，这个CNN模型通过两层卷积和池化操作有效地提取了图像的局部特征，之后通过全连接层进行高层次的特征映射，最终通过Softmax层进行分类，能够对MNIST手写数字数据集进行准确的分类。

​	以下是我利用[NN SVG](http://alexlenail.me/NN-SVG/LeNet.html#)网站画出的**我此次设计的CNN神经网络可视化模型**，貌似不太好截到报告里，我会随附件一起上传给老师，希望老师谅解哈。

![image-20250101233412222](C:\Users\DcStark\AppData\Roaming\Typora\typora-user-images\image-20250101233412222.png)

### 3.3 模型的训练与评估

#### 3.3.1 模型训练

```python
# 多次迭代
for epoch in range(0, Epoch):
    print('Training epoch {}/{}'.format(epoch + 1, Epoch))
    correct_cnt_epoch = 0  
    total_cnt_epoch = 0    
    for i, data in enumerate(train_loader, 0):
        img, label = data
        if use_gpu:
            img, label = img.cuda(), label.cuda()

        optimizer.zero_grad()
        output = net(img)
        loss = criterion(output, label)
        loss.backward()
        optimizer.step()

        _, predict = torch.max(output, 1)
        correct_cnt += (predict == label).sum()  
        correct_cnt_epoch += (predict == label).sum().item()
        total_cnt_epoch += label.size(0)  

        # 存储损失值与精度
        if i % record_interval == record_interval - 1:
            counter_temp += record_interval * batch_size
            counter.append(counter_temp)
            loss_history.append(loss.item())
            correct_history.append(correct_cnt.float().item() / (record_interval * batch_size))
            correct_cnt = 0

    # 计算并打印当前epoch的准确率
    epoch_accuracy = correct_cnt_epoch / total_cnt_epoch * 100 
    print(f"Epoch {epoch + 1} - Accuracy: {epoch_accuracy:.2f}%")
    print(f"Epoch {epoch + 1} - Final loss: {loss.item()}")
```

​	代码初始化了交叉熵损失函数（`criterion = nn.CrossEntropyLoss()`）和Adam优化器（`optimizer = torch.optim.Adam(net.parameters(), lr=learn_rate)`），前者用于**计算模型输出与真实标签之间的误差**，后者则**自适应地调整学习率**来优化模型参数。接着，定义了多个**辅助变量**，如`counter`、`loss_history`、`correct_history`，用于记录训练过程中每个epoch的进度、损失值和准确率。此外，`epoch_number`和`correct_cnt`分别用于计数当前的epoch和累计正确分类的样本数，而`record_interval = 100`指定了每100个批次记录一次损失和准确率。

​	随后，开始了训练的主要循环，通过`for epoch in range(0, Epoch)`遍历每个epoch，内层循环`for i, data in enumerate(train_loader, 0)`则逐批次获取训练数据。在每次迭代中，首先获取当前批次的图像数据和标签，并根据需要将其传输到GPU上进行加速处理。接着，通过`optimizer.zero_grad()`**清零之前的梯度**，进行**前向传播**得到模型输出，然后计算损失`loss = criterion(output, label)`，并通过`loss.backward()`计算损失函数对各参数的梯度。紧接着，使用`optimizer.step()`更新模型的参数。

​	在每个批次处理完之后，使用`torch.max(output, 1)`获取预测结果，并通过与真实标签的对比更新正确预测的样本数。在每处理完指定数量的批次（由`record_interval = 100`控制）后，记录当前的训练进度、损失值和准确率，并将其保存在`counter`、`loss_history`和`correct_history`中。每100个批次后，`correct_cnt`会被重置，准备计算下一周期的准确率。最后，代码打印出每个epoch的当前损失值以及准确率，帮助实时监控训练过程。

![image-20250101235540502](C:\Users\DcStark\AppData\Roaming\Typora\typora-user-images\image-20250101235540502.png)

#### 3.3.2 模型评估

```python
# MNIST测试集
test_dataset = datasets.MNIST(root='.', train=False, transform=transforms.ToTensor(), download=download)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset, shuffle=True, batch_size=batch_size)

# 存入迭代器
dataiter = iter(test_loader)
batch = next(dataiter)

# 训练集预测测试
start = time.time()
correct = 0
for i,data in enumerate(test_loader, 0):
    img,label = data
    if use_gpu:
        img, label = img.cuda(), label.cuda()
    output = net(img)
    _,predict = torch.max(output,1)
    correct += (predict==label).sum() 
end = time.time()

# 显示部分测试结果
if show_pic:
    imshow(torchvision.utils.make_grid(img[75:100].cpu(),nrow=5,padding=2,pad_value=1),'25张测试结果:\n'+str(predict[75:100].cpu().numpy()))

# 输出测试准确率
print('MNIST测试集识别准确率= {:.2f}'.format(correct.cpu().numpy()/len(test_dataset)*100)+'%')
```

​	在这段代码中，首先**加载了MNIST测试集**,这部分在上面介绍过，不再介绍了。

​	接下来，进行**模型预测**。在 `for` 循环中，模型对每个批次的数据进行**前向传播**，输出预测结果，并通过 `torch.max()` 获取每个样本的预测标签。通过比较预测结果与实际标签，计算预测正确的样本数。在测试过程中，如果启用了GPU加速（`use_gpu=True`），则图像和标签会被移到GPU上进行计算。每个批次的结果都会累加到 `correct` 变量中，最终计算出总的预测正确数。

​	在测试结束后，代码**计算了测试集的准确率**，并输出。通过 `correct.cpu().numpy()` 得到正确预测的数量，将其除以测试集的总样本数并乘以100，最终得出准确率的百分比。此外，若开启了图像显示（`show_pic=True`），则会展示部分测试结果图像。最后，测试的准确率被打印出来，作为模型性能的评估指标。

![image-20250101235845183](C:\Users\DcStark\AppData\Roaming\Typora\typora-user-images\image-20250101235845183.png)

### 3.4 模型优化

​	其实经过上述这么操作能够实现的准确率已经达到98%了，很不错的准确率了，那如果说还要继续优化，我有以下两个思路，是我此前在学习CV的时候涉及的：

1. 考虑对训练集进行**数据增强**，例如**随机裁剪、旋转、水平翻转**等方法，以增加训练样本的多样性，从而帮助模型更好地**泛化**，避免**过拟合**。当前的预处理方法仅使用了简单的标准化（`transforms.ToTensor()`），可以进一步扩展到更多的增强操作，以提高模型对复杂图像的适应能力。

2. He Kaiming大佬提出的**ResNet**网络，通过添加残差分析块，用大道至简的方法，提取更为丰富的特征信息，提高模型的表现。这个我曾经做过一份报告，也会随附件一起发给老师，是我初学CV的报告，当时写的比较粗糙。

![image-20250102001358572](C:\Users\DcStark\AppData\Roaming\Typora\typora-user-images\image-20250102001358572.png)

### 3.5 结果分析与可视化

#### 3.5.1 实验设计

​	为了讨论和验证不同训练轮数、学习率、隐层神经元数量、批处理大小对预测结果的影响，可以设计一个固定的实验参数，每次实验采用控制变量法，只对框架中的其中一个参数进行调整，并观察模型性能的变化。我们采用的初始参数框架如下：

> 经过实验，此框架准确率达到98.59%
>
> 学习率LR=0.001 训练轮数epoch=5 隐藏神经元数量512 批处理大小batch_size=100

![image-20250102160504958](C:\Users\DcStark\AppData\Roaming\Typora\typora-user-images\image-20250102160504958.png)

以下讨论设置的参数变化：

**训练轮数**：设置不同的训练轮数，5轮、10轮、20轮，来观察训练轮数对模型精度和收敛速度的影响。

**学习率**：设置不同的学习率，0.1、0.01、0.001、0.0001，来比较学习率对训练过程稳定性和模型最终性能的影响。

**隐层神经元数量**：通过修改全连接层的神经元数量，512、1024、2048、4096，来分析隐藏层大小对模型表现的影响。

**批处理大小**：改变批处理大小，如64、100、256等，来探讨批处理大小对训练时间、内存使用以及模型精度的影响。

#### 3.5.2 不同训练轮数的影响

| 训练轮数epoch | 准确率accuracy |
| :-----------: | :------------: |
|       5       |     98.78%     |
|      10       |     98.80%     |
|      20       |     98.58%     |

可以看到：

1. 当训练轮数从 5 增加到 10 时，测试准确率略微上升，从 98.78% 提高至 98.80%。

2. 然而，当训练轮数进一步增加到 20 时，测试准确率反而略微下降至 98.58%。

- **原因分析**
  1. **数据复杂度较低**： MNIST 数据集是手写数字分类问题，**数据特征较为简单且类别明确**。因此，模型在早期训练阶段即可达到较高的准确率，而**增加训练轮数对进一步提升模型性能的贡献有限**。
  2. **过拟合**： 当训练轮数增加至 20 时，测试准确率出现轻微下降。这可能是由于过拟合的原因，即模型在训练集上的拟合效果增强，但在测试集上的泛化能力减弱。随着训练轮数的增加，模型可能开始学习训练数据中的噪声或特定模式，从而导致泛化性能下降。
  3. **收敛速度较快**： 从实验结果可以观察到，在较少的训练轮数（如 5 或 10）时，模型已经达到了较高的准确率，这表明模型在前期训练中收敛速度较快。MNIST 数据集相对简单，且模型的参数设计较为合理，因此在少量轮数内模型已经能够很好地拟合数据。

#### 3.5.3 不同学习率的影响

| 学习率learning rate | 准确率accuracy |
| :-----------------: | :------------: |
|         0.1         |     11.35%     |
|        0.01         |     12.80%     |
|        0.001        |     98.78%     |
|       0.0001        |     98.05%     |

可以看到：

1. 当学习率为 0.1 和 0.01 时，测试准确率分别仅为 11.35% 和 12.80%，模型几乎无法有效学习。
2. 当学习率降低至 0.001 时，测试准确率显著提升至 98.78%，表明此时模型能够有效训练并达到较优的性能。
3. 进一步降低学习率至 0.0001，测试准确率略微下降至 98.05%，尽管仍然保持较高水平，但模型的训练效率可能受到了影响。

- **原因分析**
  1. **学习率过高导致模型震荡**： 当学习率为 0.1 或 0.01 时，梯度更新幅度过大，导致模型参数难以收敛甚至发生**震荡**，从而无法有效学习训练数据的特征。因此，在这两种情况下，模型的准确率较低。
  2. **适中学习率的优越性**： 当学习率为 0.001 时，模型在训练过程中能够以合适的步伐更新参数，既**避免了震荡**，又能够**较快地逼近最优解**，从而实现了最高的测试准确率。这表明 0.001 是本实验中较为适合的学习率。
  3. **学习率过低的影响**： 当学习率降至 0.0001 时，模型的收敛速度显著降低，训练过程变得缓慢。尽管最终仍能取得较高的准确率（98.05%），但**可能未能在有限的训练轮数内完全收敛至全局最优解**。

#### 3.5.4 不同隐藏层神经元的影响

| 隐藏层神经元 | 准确率accuracy |
| :----------: | :------------: |
|     512      |     98.78%     |
|     1024     |     98.90%     |
|     2048     |     98.80%     |
|     4096     |     98.38%     |

可以看到：

1. 当隐藏层神经元数量从 512 增加到 1024 时，测试准确率从 98.78% 稍微提升到 98.90%，模型的表达能力得到了增强。
2. 当隐藏层神经元数量进一步增加到 2048 时，测试准确率略微下降至 98.80%，表明此时隐藏层容量对模型性能的提升作用有限。
3. 当隐藏层神经元数量增至 4096 时，测试准确率下降至 98.38%，可能存在过拟合现象，导致模型在测试集上的表现有所退化。

- **原因分析**
  1. **模型表达能力的提升**： 隐藏层神经元数量的增加提升了模型的表达能力，使其能够拟合更加复杂的数据特征。在神经元数量从 512 增加到 1024 的过程中，这种提升较为显著，因此测试准确率略有提高。
  2. **容量过大的影响**： 当隐藏层神经元数量过多（ 2048 或 4096），模型的参数数量显著增加。这种情况下，模型在训练集上的拟合能力虽有所增强，但可能会导致模型的泛化能力下降，进而影响测试集上的表现。
  3. **过拟合现象**： 随着神经元数量的增多，模型的复杂度可能超过任务所需的范围，容易对训练集中的噪声进行拟合，导致过拟合，从而在测试集上的准确率下降。

#### 3.5.5 不同批处理大小的影响

| 批处理大小batch_size | 准确率accuracy |
| :------------------: | :------------: |
|          64          |     98.84%     |
|         100          |     98.78%     |
|         256          |     98.45%     |
|         1024         |     98.32%     |

可以看到：

1. 批处理大小为 64 时，测试集准确率最高，为 **98.84%**。

2. 随着批处理大小的增大，测试集准确率逐渐降低：当批处理大小为 100 时，准确率略微下降至 **98.78%**；当批处理大小增至 256 和 1024 时，准确率分别下降至 **98.45%** 和 **98.32%**。

- **原因分析**
  1. **小批处理对模型优化的优点**：批处理大小64能够更**频繁地更新模型参数**，使优化过程更加精细化，从而提高模型的泛化能力。这可能是批处理大小为 64 时测试集准确率最高的原因。
  2. **大批处理导致优化过程平滑**：随着批处理大小的增大，梯度估计更加接近整体梯度，使得模型优化过程趋于平滑。然而，这也可能导致模型在训练过程中陷入较优解附近，缺乏足够的探索能力，限制了模型的泛化性能。

#### 3.5.6 可视化

> 学习率LR=0.001 训练轮数epoch=5 隐藏神经元数量512 批处理大小batch_size=100

1. 训练集与测试集演示：

![image-20250102165323301](C:\Users\DcStark\AppData\Roaming\Typora\typora-user-images\image-20250102165323301.png)

2. 损失函数与准确率演示

![image-20250102165336155](C:\Users\DcStark\AppData\Roaming\Typora\typora-user-images\image-20250102165336155.png)

## 四、总结

​	此次实验以 MNIST 数据集为研究对象，旨在通过构建卷积神经网络（CNN）实现手写数字的识别，并深入探讨不同超参数设置对模型性能的影响。实验中选取训练轮数、学习率、隐层神经元数量以及批处理大小作为主要变量，对模型在测试集上的准确率表现进行详细分析和验证。

​	首先，在训练轮数的实验中，模型在训练 5 轮时已表现出较快的收敛速度，测试集准确率为 **98.78%**；当训练轮数增加至 10 时，准确率略微上升至 **98.80%**，但进一步增加训练轮数至 20 后，准确率反而下降至 **98.58%**。这一结果表明，适度增加训练轮数可以提升模型性能，但过多的训练轮数可能导致模型过拟合，从而降低其泛化能力。

​	其次，在学习率的实验中，模型的表现对学习率的选择尤为敏感。当学习率为较大的 0.1 和 0.01 时，模型的准确率仅为 **11.35%** 和 **12.80%**，说明学习率过大导致了训练过程的震荡甚至无法收敛；而当学习率降低至 0.001 和 0.0001 时，模型分别实现了 **98.78%** 和 **98.05%** 的较高准确率，表明较小的学习率更适合于模型的稳定优化，但过小的学习率可能减缓收敛速度。因此，学习率的选择需要在快速收敛和稳定优化之间取得平衡。

​	在隐层神经元数量的实验中，随着神经元数量的增加，模型的准确率从 **98.78%**（512 个神经元）略微上升至 **98.90%**（1024 个神经元），但当隐层神经元数量继续增加至 2048 和 4096 时，准确率分别下降至 **98.80%** 和 **98.38%**。这一现象表明，适当增加隐层神经元数量能够提升模型的学习能力，但过多的神经元可能导致模型复杂度过高，增加过拟合风险，从而对测试性能产生负面影响。

​	在批处理大小的实验中，批处理大小为 64 时，模型实现了最高的测试准确率 **98.84%**，随后随着批处理大小增大至 100、256 和 1024，准确率分别下降至 **98.78%**、**98.45%** 和 **98.32%**。分析认为，较小的批处理大小能够更频繁地更新模型参数，优化过程更为精细，从而提升模型的泛化能力；而较大的批处理大小虽然在计算效率上占优，但优化过程趋于平滑，可能限制模型对数据特征的捕获能力，最终导致泛化性能的下降。

​	综上所述，本实验通过对超参数的多维度调整，深入探讨了训练轮数、学习率、隐层神经元数量和批处理大小对模型预测结果的影响。结果表明，不同超参数对模型性能的影响存在复杂的权衡关系，合理的超参数设置是模型性能优化的关键。

## 五、附件

附件一：`mnist_pytorch.py`手写数字识别主程序

附件二：

